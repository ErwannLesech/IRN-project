{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a7a466-0710-425b-8991-d23dc09e4082",
   "metadata": {},
   "source": [
    "# TP 1\n",
    "## Groupe des amateurs de vin\n",
    "\n",
    "### Reconnaissance de vin.\n",
    "\n",
    "Dans ce projet, nous allons faire de la reconnaissance de vins.\n",
    "\n",
    "Source des données : https://huggingface.co/datasets/katossky/wine-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ba8e96-0d57-4481-9b28-c26530f1243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "from torch.optim import Adam, SGD, Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a9b8a-aa0a-4d2d-8859-ba68b0de3915",
   "metadata": {},
   "source": [
    "### Étape 1 : Charger les données\n",
    "\n",
    "En PyTorch, les données doivent être transmise au réseau de neurones à l'aide d'un loader. La première étape est de créer une classe de type `Dataset` que la fonction `DataLoader` prend en argument. La classe doit au moins posséder les trois routine `__init__`, `__len__` et `__getitem__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed7918b-d6b4-4c12-a5f4-5ef129da89dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label  alcohol  malic acid   ash  alcalinity of ash  magnesium  \\\n",
      "0        1    14.23        1.71  2.43               15.6        127   \n",
      "1        1    13.20        1.78  2.14               11.2        100   \n",
      "2        1    13.16        2.36  2.67               18.6        101   \n",
      "3        1    14.37        1.95  2.50               16.8        113   \n",
      "4        1    13.24        2.59  2.87               21.0        118   \n",
      "..     ...      ...         ...   ...                ...        ...   \n",
      "173      3    13.71        5.65  2.45               20.5         95   \n",
      "174      3    13.40        3.91  2.48               23.0        102   \n",
      "175      3    13.27        4.28  2.26               20.0        120   \n",
      "176      3    13.17        2.59  2.37               20.0        120   \n",
      "177      3    14.13        4.10  2.74               24.5         96   \n",
      "\n",
      "     total phenols  flavanoids  nonflavanoid phenols  proanthocyanins  \\\n",
      "0             2.80        3.06                  0.28             2.29   \n",
      "1             2.65        2.76                  0.26             1.28   \n",
      "2             2.80        3.24                  0.30             2.81   \n",
      "3             3.85        3.49                  0.24             2.18   \n",
      "4             2.80        2.69                  0.39             1.82   \n",
      "..             ...         ...                   ...              ...   \n",
      "173           1.68        0.61                  0.52             1.06   \n",
      "174           1.80        0.75                  0.43             1.41   \n",
      "175           1.59        0.69                  0.43             1.35   \n",
      "176           1.65        0.68                  0.53             1.46   \n",
      "177           2.05        0.76                  0.56             1.35   \n",
      "\n",
      "     color intensity   hue  OD280/OD315 of diluted wines  proline  \n",
      "0               5.64  1.04                          3.92     1065  \n",
      "1               4.38  1.05                          3.40     1050  \n",
      "2               5.68  1.03                          3.17     1185  \n",
      "3               7.80  0.86                          3.45     1480  \n",
      "4               4.32  1.04                          2.93      735  \n",
      "..               ...   ...                           ...      ...  \n",
      "173             7.70  0.64                          1.74      740  \n",
      "174             7.30  0.70                          1.56      750  \n",
      "175            10.20  0.59                          1.56      835  \n",
      "176             9.30  0.60                          1.62      840  \n",
      "177             9.20  0.61                          1.60      560  \n",
      "\n",
      "[178 rows x 14 columns]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Sample features: tensor([ 1.5186, -0.5622,  0.2321, -1.1696,  1.9139,  0.8090,  1.0348, -0.6596,\n",
      "         1.2249,  0.2517,  0.3622,  1.8479,  1.0130])\n",
      "Sample label: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wineDataset = pd.read_csv(\"wine-dataset/wine.csv\")\n",
    "\n",
    "print(wineDataset)\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "        \n",
    "        self.labels = self.data.iloc[:, 0].values  # first column is label\n",
    "        self.features = self.data.iloc[:, 1:].values  # All other columns are features\n",
    "        \n",
    "        # Normalize the features (if needed)\n",
    "        self.features = (self.features - self.features.mean(axis=0)) / self.features.std(axis=0)\n",
    "        # Turns labels into indices from 0 to 2 for CrossEntropyLoss\n",
    "        self.labels = self.labels - 1\n",
    "        # Ensure labels are binary (0 and 1)\n",
    "        '''label_min = self.labels.min()\n",
    "        label_max = self.labels.max()\n",
    "        if label_max - label_min != 0:\n",
    "            self.labels = (self.labels - label_min) / (label_max - label_min)\n",
    "        else:\n",
    "            self.labels = self.labels * 0'''\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.int64)  # or torch.float32 if needed\n",
    "        return features, label\n",
    "\n",
    "# Instantiate the dataset\n",
    "wine_dataset = WineDataset(wineDataset)\n",
    "\n",
    "# Print the labels to verify\n",
    "print(wine_dataset.labels)\n",
    "\n",
    "# Example: Fetch a sample to verify\n",
    "features, label = wine_dataset[0]\n",
    "print(f'Sample features: {features}')\n",
    "print(f'Sample label: {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70aed5-b312-4f3a-be3d-18d0913c3377",
   "metadata": {},
   "source": [
    "### Etape 2 : Construire le model du réseau de neurones\n",
    "\n",
    "Dans cette partie, nous élaborons notre model afin qu'il puisse être entrainé sur notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57679e46-57fe-4083-a4bf-146589b7f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "wineModel = torch.nn.Sequential(torch.nn.Linear(13,64),\n",
    "                                torch.nn.ReLU(),\n",
    "                                torch.nn.Linear(64,13),\n",
    "                                torch.nn.ReLU(),\n",
    "                                torch.nn.Linear(13,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99fb53-415d-4583-923e-9f855b8525e3",
   "metadata": {},
   "source": [
    "### Étape 3 : création des loader d'entraintement, de validation et de test\n",
    "\n",
    "Ici on choisit un batch de 20.\n",
    "\n",
    "Pour une taille de batch de 20, on coupera le jeu de donnée de la manière suivante : \n",
    "- Entrainement de la partie 1 à la partie 120,\n",
    "- Validation de la partie 120 à la partie 160,\n",
    "- Test de la partie 160 à la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5505f134-531e-433a-8c8f-15654a75f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for train, validation, and test datasets\n",
    "wineDataset = wineDataset.sample(n=len(wineDataset))\n",
    "train_path = wineDataset.iloc[:120]\n",
    "val_path = wineDataset.iloc[120:160]\n",
    "test_path = wineDataset.iloc[160:]\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = WineDataset(train_path)\n",
    "val_dataset = WineDataset(val_path)\n",
    "test_dataset = WineDataset(test_path)\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a92d1-d1a1-4336-9c64-d81e5fa39b78",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce7a62e-7069-4be5-a99b-e28e0a5496d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_loss_accuracy(nepoch, tloss, vloss, accuracy, best_tloss, best_vloss, best_accuracy):\n",
    "    print (\"{:<6} {:<15} {:<17} {:<15} {:<20} {:<22} {:<15}\".format(nepoch, tloss, vloss, accuracy, best_tloss, best_vloss, best_accuracy))\n",
    "\n",
    "def learning(nepoch, model, crit, optim, batchsize, trainingloader, validationloader, writer):\n",
    "    best_tloss = 100.\n",
    "    best_vloss = 100.\n",
    "    best_accuracy = 0.\n",
    "    \n",
    "    Print_loss_accuracy('Epoch', 'training loss', 'validation loss', 'accuracy', 'best train loss', 'best validation loss', 'best accuracy')\n",
    "    \n",
    "    for epoch in range(nepoch):\n",
    "        tloss = 0.\n",
    "        vloss = 0.\n",
    "        correct_test = 0\n",
    "        model.train()\n",
    "        \n",
    "        for features, labels in trainingloader:\n",
    "            optim.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = crit(outputs, labels)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            tloss += loss.item() * features.size(0)\n",
    "        \n",
    "        tloss /= len(trainingloader.dataset)\n",
    "        model.eval()\n",
    "        \n",
    "        for features, labels in validationloader:\n",
    "            predicted = model(features)\n",
    "            _, predicted_labels = torch.max(predicted, 1)\n",
    "            correct_test += (predicted_labels == labels).sum().item()\n",
    "            loss = crit(predicted, labels)  # Ensure labels has the same shape as predicted\n",
    "            vloss += loss.item() * features.size(0)\n",
    "        \n",
    "        vloss /= len(validationloader.dataset)\n",
    "        accuracy = 100 * correct_test / len(validationloader.dataset)\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar('Loss/Train', tloss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', vloss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', accuracy, epoch)\n",
    "\n",
    "        if accuracy >= best_accuracy:\n",
    "            torch.save(model.state_dict(), f\"best_model.pth\")\n",
    "            best_accuracy = accuracy\n",
    "        if vloss <= best_vloss:\n",
    "            best_vloss = vloss\n",
    "        if tloss <= best_tloss:\n",
    "            best_tloss = tloss\n",
    "        \n",
    "        Print_loss_accuracy(epoch + 1, \n",
    "                            np.round(tloss, 8), \n",
    "                            np.round(vloss, 8), \n",
    "                            np.round(accuracy, 8), \n",
    "                            np.round(best_tloss, 8), \n",
    "                            np.round(best_vloss, 8), \n",
    "                            np.round(best_accuracy, 8))\n",
    "\n",
    "    # Ensure all pending events have been written to disk\n",
    "    writer.flush()\n",
    "\n",
    "    # Close the SummaryWriter\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89aca5-1e8f-4181-985b-3fc52cf7afb6",
   "metadata": {},
   "source": [
    "### Test model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed3c5c99-d923-4fee-bd1f-55c268de71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testmodel(modelfile, crit, testloader):\n",
    "    # Load the trained model\n",
    "    model = torch.load(modelfile)\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    plt.figure(dpi=300)\n",
    "    ct = 1\n",
    "    for features, labels in testloader:\n",
    "        image = features[0].permute(1, 2, 0)\n",
    "        plt.subplot(1, len(test_loader.sampler), ct)\n",
    "        plt.imshow(image)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        predicted = model(features).squeeze(dim=1)\n",
    "        loss = crit(predicted, labels.squeeze(dim=1))\n",
    "        plt.title('True label : {} \\n Predicted label : {} \\n Test loss : {}'.format(labels.squeeze().detach().numpy(), \n",
    "                                                                       predicted.round().detach().numpy(),\n",
    "                                                                       np.round(test_loss.item(), 2)),\n",
    "                  fontsize=6)\n",
    "        ct += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9542f5-70d9-4cc7-af70-821ce8c2b239",
   "metadata": {},
   "source": [
    "### Etape 4 : Choisir une fonction coût"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc3772a-09ec-4085-96aa-8b0635e15fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(wineModel.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65919506-ceec-4f9b-b0d8-947fa8fbbc54",
   "metadata": {},
   "source": [
    "### Training & Test\n",
    "\n",
    "C'est le moment de s'amuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0be17d25-af7e-4ffe-a4f9-e8d717426070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with optimizer: SGD, learning rate: 0.001\n",
      "Epoch  training loss   validation loss   accuracy        best train loss      best validation loss   best accuracy  \n",
      "1      1.09701151      1.12452078        32.5            1.09701151           1.12452078             32.5           \n",
      "2      1.09651037      1.12409145        32.5            1.09651037           1.12409145             32.5           \n",
      "3      1.09602672      1.12366313        32.5            1.09602672           1.12366313             32.5           \n",
      "4      1.09551585      1.12323153        32.5            1.09551585           1.12323153             32.5           \n",
      "5      1.09504086      1.12279898        32.5            1.09504086           1.12279898             32.5           \n",
      "6      1.09454219      1.12236893        32.5            1.09454219           1.12236893             32.5           \n",
      "7      1.09407379      1.12193984        32.5            1.09407379           1.12193984             32.5           \n",
      "8      1.09355106      1.12151086        32.5            1.09355106           1.12151086             32.5           \n",
      "9      1.09306403      1.12108171        32.5            1.09306403           1.12108171             32.5           \n",
      "10     1.09257537      1.12065345        32.5            1.09257537           1.12065345             32.5           \n",
      "11     1.09209275      1.12022787        32.5            1.09209275           1.12022787             32.5           \n",
      "12     1.09161168      1.11980492        32.5            1.09161168           1.11980492             32.5           \n",
      "13     1.09113695      1.11938274        32.5            1.09113695           1.11938274             32.5           \n",
      "14     1.09066902      1.11896122        32.5            1.09066902           1.11896122             32.5           \n",
      "15     1.09017944      1.11853892        32.5            1.09017944           1.11853892             32.5           \n",
      "16     1.08970952      1.11811727        32.5            1.08970952           1.11811727             32.5           \n",
      "17     1.08923062      1.11769611        32.5            1.08923062           1.11769611             32.5           \n",
      "18     1.08874959      1.11727548        32.5            1.08874959           1.11727548             32.5           \n",
      "19     1.08825411      1.11685532        32.5            1.08825411           1.11685532             32.5           \n",
      "20     1.08780108      1.11643583        32.5            1.08780108           1.11643583             32.5           \n",
      "Training with optimizer: SGD, learning rate: 0.01\n",
      "Epoch  training loss   validation loss   accuracy        best train loss      best validation loss   best accuracy  \n",
      "1      1.09539223      1.12066233        32.5            1.09539223           1.12066233             32.5           \n",
      "2      1.09074547      1.11641926        32.5            1.09074547           1.11641926             32.5           \n",
      "3      1.08568631      1.11227018        32.5            1.08568631           1.11227018             32.5           \n",
      "4      1.08116082      1.10809439        32.5            1.08116082           1.10809439             32.5           \n",
      "5      1.07645996      1.10401696        32.5            1.07645996           1.10401696             32.5           \n",
      "6      1.07186937      1.09998232        32.5            1.07186937           1.09998232             32.5           \n",
      "7      1.06709734      1.09581345        32.5            1.06709734           1.09581345             32.5           \n",
      "8      1.06205901      1.09155899        35.0            1.06205901           1.09155899             35.0           \n",
      "9      1.05722032      1.08722401        35.0            1.05722032           1.08722401             35.0           \n",
      "10     1.05228923      1.08252108        35.0            1.05228923           1.08252108             35.0           \n",
      "11     1.04695517      1.07755715        35.0            1.04695517           1.07755715             35.0           \n",
      "12     1.0416347       1.07237774        35.0            1.0416347            1.07237774             35.0           \n",
      "13     1.03597218      1.06702083        32.5            1.03597218           1.06702083             35.0           \n",
      "14     1.03005335      1.06138563        35.0            1.03005335           1.06138563             35.0           \n",
      "15     1.02398724      1.05550945        35.0            1.02398724           1.05550945             35.0           \n",
      "16     1.01787908      1.04943436        35.0            1.01787908           1.04943436             35.0           \n",
      "17     1.01119914      1.04313368        40.0            1.01119914           1.04313368             40.0           \n",
      "18     1.00432646      1.03660864        40.0            1.00432646           1.03660864             40.0           \n",
      "19     0.99740775      1.02977657        40.0            0.99740775           1.02977657             40.0           \n",
      "20     0.98959136      1.02247119        42.5            0.98959136           1.02247119             42.5           \n",
      "Training with optimizer: Adagrad, learning rate: 0.001\n",
      "Epoch  training loss   validation loss   accuracy        best train loss      best validation loss   best accuracy  \n",
      "1      1.08924709      1.10907388        32.5            1.08924709           1.10907388             32.5           \n",
      "2      1.07505029      1.10029751        32.5            1.07505029           1.10029751             32.5           \n",
      "3      1.06579387      1.0931468         35.0            1.06579387           1.0931468              35.0           \n",
      "4      1.05760342      1.08679742        35.0            1.05760342           1.08679742             35.0           \n",
      "5      1.05034         1.08100075        35.0            1.05034              1.08100075             35.0           \n",
      "6      1.04358086      1.07557219        35.0            1.04358086           1.07557219             35.0           \n",
      "7      1.03695951      1.07033229        35.0            1.03695951           1.07033229             35.0           \n",
      "8      1.03055062      1.06524563        35.0            1.03055062           1.06524563             35.0           \n",
      "9      1.02449335      1.06021231        35.0            1.02449335           1.06021231             35.0           \n",
      "10     1.01859106      1.05510342        37.5            1.01859106           1.05510342             37.5           \n",
      "11     1.01281225      1.05004567        37.5            1.01281225           1.05004567             37.5           \n",
      "12     1.0071585       1.04500616        37.5            1.0071585            1.04500616             37.5           \n",
      "13     1.00171496      1.04001862        40.0            1.00171496           1.04001862             40.0           \n",
      "14     0.99619612      1.03500181        40.0            0.99619612           1.03500181             40.0           \n",
      "15     0.99074185      1.02999854        40.0            0.99074185           1.02999854             40.0           \n",
      "16     0.98527399      1.02510715        42.5            0.98527399           1.02510715             42.5           \n",
      "17     0.97991391      1.02030849        40.0            0.97991391           1.02030849             42.5           \n",
      "18     0.97464565      1.01560187        40.0            0.97464565           1.01560187             42.5           \n",
      "19     0.96937582      1.01093945        40.0            0.96937582           1.01093945             42.5           \n",
      "20     0.96418096      1.00632465        45.0            0.96418096           1.00632465             45.0           \n",
      "Training with optimizer: Adagrad, learning rate: 0.01\n",
      "Epoch  training loss   validation loss   accuracy        best train loss      best validation loss   best accuracy  \n",
      "1      0.99158489      0.896312          57.5            0.99158489           0.896312               57.5           \n",
      "2      0.7578098       0.7028262         67.5            0.7578098            0.7028262              67.5           \n",
      "3      0.57313794      0.5560059         85.0            0.57313794           0.5560059              85.0           \n",
      "4      0.43122376      0.45222594        87.5            0.43122376           0.45222594             87.5           \n",
      "5      0.32968101      0.36598107        92.5            0.32968101           0.36598107             92.5           \n",
      "6      0.24893088      0.30048335        92.5            0.24893088           0.30048335             92.5           \n",
      "7      0.19285671      0.25775781        95.0            0.19285671           0.25775781             95.0           \n",
      "8      0.15289177      0.22731753        95.0            0.15289177           0.22731753             95.0           \n",
      "9      0.12649309      0.20602216        95.0            0.12649309           0.20602216             95.0           \n",
      "10     0.10577051      0.19096954        95.0            0.10577051           0.19096954             95.0           \n",
      "11     0.09032964      0.1803329         95.0            0.09032964           0.1803329              95.0           \n",
      "12     0.07984181      0.17014623        95.0            0.07984181           0.17014623             95.0           \n",
      "13     0.06953032      0.16258561        95.0            0.06953032           0.16258561             95.0           \n",
      "14     0.062087        0.15621315        95.0            0.062087             0.15621315             95.0           \n",
      "15     0.0556944       0.15174657        95.0            0.0556944            0.15174657             95.0           \n",
      "16     0.05044937      0.14750914        95.0            0.05044937           0.14750914             95.0           \n",
      "17     0.04608689      0.14392969        95.0            0.04608689           0.14392969             95.0           \n",
      "18     0.04256041      0.14083363        95.0            0.04256041           0.14083363             95.0           \n",
      "19     0.03925763      0.13856512        95.0            0.03925763           0.13856512             95.0           \n",
      "20     0.03631669      0.13662378        95.0            0.03631669           0.13662378             95.0           \n",
      "Training with optimizer: Adam, learning rate: 0.001\n",
      "Epoch  training loss   validation loss   accuracy        best train loss      best validation loss   best accuracy  \n",
      "1      1.08565537      1.09681082        35.0            1.08565537           1.09681082             35.0           \n",
      "2      1.05364628      1.06941557        35.0            1.05364628           1.06941557             35.0           \n",
      "3      1.02117005      1.0394153         37.5            1.02117005           1.0394153              37.5           \n",
      "4      0.98473438      1.00280041        45.0            0.98473438           1.00280041             45.0           \n",
      "5      0.93970053      0.96076697        52.5            0.93970053           0.96076697             52.5           \n",
      "6      0.88914513      0.91059017        55.0            0.88914513           0.91059017             55.0           \n",
      "7      0.83033882      0.85172865        52.5            0.83033882           0.85172865             55.0           \n",
      "8      0.76453253      0.78773338        67.5            0.76453253           0.78773338             67.5           \n",
      "9      0.69539044      0.71939069        77.5            0.69539044           0.71939069             77.5           \n",
      "10     0.62283829      0.64998806        85.0            0.62283829           0.64998806             85.0           \n",
      "11     0.55044099      0.58207709        85.0            0.55044099           0.58207709             85.0           \n",
      "12     0.47700556      0.51554668        87.5            0.47700556           0.51554668             87.5           \n",
      "13     0.40822035      0.45073119        92.5            0.40822035           0.45073119             92.5           \n",
      "14     0.34373875      0.39221677        92.5            0.34373875           0.39221677             92.5           \n",
      "15     0.28492181      0.34272583        92.5            0.28492181           0.34272583             92.5           \n",
      "16     0.23592457      0.3018645         92.5            0.23592457           0.3018645              92.5           \n",
      "17     0.19514159      0.27107964        92.5            0.19514159           0.27107964             92.5           \n",
      "18     0.16182665      0.24360017        92.5            0.16182665           0.24360017             92.5           \n",
      "19     0.13554777      0.22102656        92.5            0.13554777           0.22102656             92.5           \n",
      "20     0.11512968      0.20371366        95.0            0.11512968           0.20371366             95.0           \n",
      "Training with optimizer: Adam, learning rate: 0.01\n",
      "Epoch  training loss   validation loss   accuracy        best train loss      best validation loss   best accuracy  \n",
      "1      0.96073938      0.75027996        77.5            0.96073938           0.75027996             77.5           \n",
      "2      0.48819345      0.30487895        95.0            0.48819345           0.30487895             95.0           \n",
      "3      0.1420047       0.17047715        95.0            0.1420047            0.17047715             95.0           \n",
      "4      0.05398781      0.1313934         95.0            0.05398781           0.1313934              95.0           \n",
      "5      0.02401031      0.18623611        90.0            0.02401031           0.1313934              95.0           \n",
      "6      0.02523361      0.132704          95.0            0.02401031           0.1313934              95.0           \n",
      "7      0.00421013      0.11175871        95.0            0.00421013           0.11175871             95.0           \n",
      "8      0.0030546       0.09711991        95.0            0.0030546            0.09711991             95.0           \n",
      "9      0.00184744      0.09170434        95.0            0.00184744           0.09170434             95.0           \n",
      "10     0.00075389      0.09365546        95.0            0.00075389           0.09170434             95.0           \n",
      "11     0.00051634      0.09977255        95.0            0.00051634           0.09170434             95.0           \n",
      "12     0.0004367       0.10862579        95.0            0.0004367            0.09170434             95.0           \n",
      "13     0.00037176      0.11360397        92.5            0.00037176           0.09170434             95.0           \n",
      "14     0.00034109      0.11715383        92.5            0.00034109           0.09170434             95.0           \n",
      "15     0.00031565      0.11724513        95.0            0.00031565           0.09170434             95.0           \n",
      "16     0.00028695      0.11854464        95.0            0.00028695           0.09170434             95.0           \n",
      "17     0.00026807      0.12102113        95.0            0.00026807           0.09170434             95.0           \n",
      "18     0.0002527       0.12129868        95.0            0.0002527            0.09170434             95.0           \n",
      "19     0.00024302      0.12164677        95.0            0.00024302           0.09170434             95.0           \n",
      "20     0.00023178      0.12268545        95.0            0.00023178           0.09170434             95.0           \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "LEARNING_RATES = [0.001, 0.01]\n",
    "OPTIMIZERS = {\n",
    "    \"SGD\": SGD,\n",
    "    \"Adagrad\": Adagrad,\n",
    "    \"Adam\": Adam\n",
    "}\n",
    "\n",
    "# Copy the initial model to ensure the same starting weights for each run\n",
    "initial_model_state = copy.deepcopy(wineModel.state_dict())\n",
    "\n",
    "# Loop through configurations and train model\n",
    "for optimizer_name, optimizer_class in OPTIMIZERS.items():\n",
    "    for lr in LEARNING_RATES:\n",
    "        # Reset the model to initial state\n",
    "        wineModel.load_state_dict(initial_model_state)\n",
    "\n",
    "        # Create a new instance of the optimizer for each run\n",
    "        optimizer = optimizer_class(wineModel.parameters(), lr=lr)\n",
    "\n",
    "        # Create a SummaryWriter with a unique run name\n",
    "        run_name = f\"optimizer={optimizer_name}_lr={lr}_epochs={EPOCHS}\"\n",
    "        writer = SummaryWriter(log_dir=f'runs/{run_name}')\n",
    "\n",
    "        print(f\"Training with optimizer: {optimizer_name}, learning rate: {lr}\")\n",
    "        \n",
    "        # Train the model with the current configuration\n",
    "        learning(EPOCHS, wineModel, criterion, optimizer, BATCH_SIZE, train_loader, val_loader, writer)\n",
    "\n",
    "        # Save the best model for the current configuration\n",
    "        torch.save(wineModel.state_dict(), f\"best_models/best_model_{run_name}.pth\")\n",
    "\n",
    "# learning(20, wineModel, criterion, optimizer, BATCH_SIZE, train_loader, val_loader)\n",
    "# torch.save(wineModel.state_dict(), \"best_model.pth\")\n",
    "# print(len(val_loader.dataset))\n",
    "# Testmodel(\"best_model.pth\", criterion, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b747b-8dfe-4257-b1f4-338163b53aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da8f48b-bcd3-4c04-bb7e-8bd456669178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc3d3d-816a-4328-adc4-b864e0c36aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
