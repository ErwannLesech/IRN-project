{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a7a466-0710-425b-8991-d23dc09e4082",
   "metadata": {},
   "source": [
    "# TP 1\n",
    "## Groupe des amateurs de vin\n",
    "\n",
    "### Reconnaissance de vin.\n",
    "\n",
    "Dans ce projet, nous allons faire de la reconnaissance de vins.\n",
    "\n",
    "Source des données : https://huggingface.co/datasets/katossky/wine-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ba8e96-0d57-4481-9b28-c26530f1243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "from torch.optim import Adam, SGD, Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a9b8a-aa0a-4d2d-8859-ba68b0de3915",
   "metadata": {},
   "source": [
    "### Étape 1 : Charger les données\n",
    "\n",
    "En PyTorch, les données doivent être transmise au réseau de neurones à l'aide d'un loader. La première étape est de créer une classe de type `Dataset` que la fonction `DataLoader` prend en argument. La classe doit au moins posséder les trois routine `__init__`, `__len__` et `__getitem__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed7918b-d6b4-4c12-a5f4-5ef129da89dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label  alcohol  malic acid   ash  alcalinity of ash  magnesium  \\\n",
      "0        1    14.23        1.71  2.43               15.6        127   \n",
      "1        1    13.20        1.78  2.14               11.2        100   \n",
      "2        1    13.16        2.36  2.67               18.6        101   \n",
      "3        1    14.37        1.95  2.50               16.8        113   \n",
      "4        1    13.24        2.59  2.87               21.0        118   \n",
      "..     ...      ...         ...   ...                ...        ...   \n",
      "173      3    13.71        5.65  2.45               20.5         95   \n",
      "174      3    13.40        3.91  2.48               23.0        102   \n",
      "175      3    13.27        4.28  2.26               20.0        120   \n",
      "176      3    13.17        2.59  2.37               20.0        120   \n",
      "177      3    14.13        4.10  2.74               24.5         96   \n",
      "\n",
      "     total phenols  flavanoids  nonflavanoid phenols  proanthocyanins  \\\n",
      "0             2.80        3.06                  0.28             2.29   \n",
      "1             2.65        2.76                  0.26             1.28   \n",
      "2             2.80        3.24                  0.30             2.81   \n",
      "3             3.85        3.49                  0.24             2.18   \n",
      "4             2.80        2.69                  0.39             1.82   \n",
      "..             ...         ...                   ...              ...   \n",
      "173           1.68        0.61                  0.52             1.06   \n",
      "174           1.80        0.75                  0.43             1.41   \n",
      "175           1.59        0.69                  0.43             1.35   \n",
      "176           1.65        0.68                  0.53             1.46   \n",
      "177           2.05        0.76                  0.56             1.35   \n",
      "\n",
      "     color intensity   hue  OD280/OD315 of diluted wines  proline  \n",
      "0               5.64  1.04                          3.92     1065  \n",
      "1               4.38  1.05                          3.40     1050  \n",
      "2               5.68  1.03                          3.17     1185  \n",
      "3               7.80  0.86                          3.45     1480  \n",
      "4               4.32  1.04                          2.93      735  \n",
      "..               ...   ...                           ...      ...  \n",
      "173             7.70  0.64                          1.74      740  \n",
      "174             7.30  0.70                          1.56      750  \n",
      "175            10.20  0.59                          1.56      835  \n",
      "176             9.30  0.60                          1.62      840  \n",
      "177             9.20  0.61                          1.60      560  \n",
      "\n",
      "[178 rows x 14 columns]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Sample features: tensor([ 1.5186, -0.5622,  0.2321, -1.1696,  1.9139,  0.8090,  1.0348, -0.6596,\n",
      "         1.2249,  0.2517,  0.3622,  1.8479,  1.0130])\n",
      "Sample label: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wineDataset = pd.read_csv(\"wine-dataset/wine.csv\")\n",
    "\n",
    "print(wineDataset)\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "        \n",
    "        self.labels = self.data.iloc[:, 0].values  # first column is label\n",
    "        self.features = self.data.iloc[:, 1:].values  # All other columns are features\n",
    "        \n",
    "        # Normalize the features (if needed)\n",
    "        self.features = (self.features - self.features.mean(axis=0)) / self.features.std(axis=0)\n",
    "        # Turns labels into indices from 0 to 2 for CrossEntropyLoss\n",
    "        self.labels = self.labels - 1\n",
    "        # Ensure labels are binary (0 and 1)\n",
    "        '''label_min = self.labels.min()\n",
    "        label_max = self.labels.max()\n",
    "        if label_max - label_min != 0:\n",
    "            self.labels = (self.labels - label_min) / (label_max - label_min)\n",
    "        else:\n",
    "            self.labels = self.labels * 0'''\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.int64)  # or torch.float32 if needed\n",
    "        return features, label\n",
    "\n",
    "# Instantiate the dataset\n",
    "wine_dataset = WineDataset(wineDataset)\n",
    "\n",
    "# Print the labels to verify\n",
    "print(wine_dataset.labels)\n",
    "\n",
    "# Example: Fetch a sample to verify\n",
    "features, label = wine_dataset[0]\n",
    "print(f'Sample features: {features}')\n",
    "print(f'Sample label: {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70aed5-b312-4f3a-be3d-18d0913c3377",
   "metadata": {},
   "source": [
    "### Etape 2 : Construire le model du réseau de neurones\n",
    "\n",
    "Dans cette partie, nous élaborons notre model afin qu'il puisse être entrainé sur notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57679e46-57fe-4083-a4bf-146589b7f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "wineModel = torch.nn.Sequential(torch.nn.Linear(13,64),\n",
    "                                torch.nn.ReLU(),\n",
    "                                torch.nn.Linear(64,13),\n",
    "                                torch.nn.ReLU(),\n",
    "                                torch.nn.Linear(13,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99fb53-415d-4583-923e-9f855b8525e3",
   "metadata": {},
   "source": [
    "### Étape 3 : création des loader d'entraintement, de validation et de test\n",
    "\n",
    "Ici on choisit un batch de 20.\n",
    "\n",
    "Pour une taille de batch de 20, on coupera le jeu de donnée de la manière suivante : \n",
    "- Entrainement de la partie 1 à la partie 120,\n",
    "- Validation de la partie 120 à la partie 160,\n",
    "- Test de la partie 160 à la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5505f134-531e-433a-8c8f-15654a75f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for train, validation, and test datasets\n",
    "wineDataset = wineDataset.sample(n=len(wineDataset))\n",
    "train_path = wineDataset.iloc[:120]\n",
    "val_path = wineDataset.iloc[120:160]\n",
    "test_path = wineDataset.iloc[160:]\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = WineDataset(train_path)\n",
    "val_dataset = WineDataset(val_path)\n",
    "test_dataset = WineDataset(test_path)\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a92d1-d1a1-4336-9c64-d81e5fa39b78",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce7a62e-7069-4be5-a99b-e28e0a5496d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_loss_accuracy(nepoch, tloss, vloss, accuracy, best_tloss, best_vloss, best_accuracy):\n",
    "    print (\"{:<6} {:<15} {:<17} {:<15} {:<20} {:<22} {:<15}\".format(nepoch, tloss, vloss, accuracy, best_tloss, best_vloss, best_accuracy))\n",
    "\n",
    "def learning(nepoch, model, crit, optim, batchsize, trainingloader, validationloader, writer):\n",
    "    best_tloss = 100.\n",
    "    best_vloss = 100.\n",
    "    best_accuracy = 0.\n",
    "    \n",
    "    Print_loss_accuracy('Epoch', 'training loss', 'validation loss', 'accuracy', 'best train loss', 'best validation loss', 'best accuracy')\n",
    "    \n",
    "    for epoch in range(nepoch):\n",
    "        tloss = 0.\n",
    "        vloss = 0.\n",
    "        correct_test = 0\n",
    "        model.train()\n",
    "        \n",
    "        for features, labels in trainingloader:\n",
    "            optim.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = crit(outputs, labels)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            tloss += loss.item() * features.size(0)\n",
    "        \n",
    "        tloss /= len(trainingloader.dataset)\n",
    "        model.eval()\n",
    "        \n",
    "        for features, labels in validationloader:\n",
    "            predicted = model(features)\n",
    "            _, predicted_labels = torch.max(predicted, 1)\n",
    "            correct_test += (predicted_labels == labels).sum().item()\n",
    "            loss = crit(predicted, labels)  # Ensure labels has the same shape as predicted\n",
    "            vloss += loss.item() * features.size(0)\n",
    "        \n",
    "        vloss /= len(validationloader.dataset)\n",
    "        accuracy = 100 * correct_test / len(validationloader.dataset)\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar('Loss/Train', tloss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', vloss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', accuracy, epoch)\n",
    "\n",
    "        if accuracy >= best_accuracy:\n",
    "            torch.save(model.state_dict(), f\"best_model.pth\")\n",
    "            best_accuracy = accuracy\n",
    "        if vloss <= best_vloss:\n",
    "            best_vloss = vloss\n",
    "        if tloss <= best_tloss:\n",
    "            best_tloss = tloss\n",
    "        \n",
    "        Print_loss_accuracy(epoch + 1, \n",
    "                            np.round(tloss, 8), \n",
    "                            np.round(vloss, 8), \n",
    "                            np.round(accuracy, 8), \n",
    "                            np.round(best_tloss, 8), \n",
    "                            np.round(best_vloss, 8), \n",
    "                            np.round(best_accuracy, 8))\n",
    "\n",
    "    # Ensure all pending events have been written to disk\n",
    "    writer.flush()\n",
    "\n",
    "    # Close the SummaryWriter\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89aca5-1e8f-4181-985b-3fc52cf7afb6",
   "metadata": {},
   "source": [
    "### Test model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed3c5c99-d923-4fee-bd1f-55c268de71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testmodel(modelfile, crit, testloader):\n",
    "    # Load the trained model\n",
    "    model = torch.load(modelfile)\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    plt.figure(dpi=300)\n",
    "    ct = 1\n",
    "    for features, labels in testloader:\n",
    "        image = features[0].permute(1, 2, 0)\n",
    "        plt.subplot(1, len(test_loader.sampler), ct)\n",
    "        plt.imshow(image)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        predicted = model(features).squeeze(dim=1)\n",
    "        loss = crit(predicted, labels.squeeze(dim=1))\n",
    "        plt.title('True label : {} \\n Predicted label : {} \\n Test loss : {}'.format(labels.squeeze().detach().numpy(), \n",
    "                                                                       predicted.round().detach().numpy(),\n",
    "                                                                       np.round(test_loss.item(), 2)),\n",
    "                  fontsize=6)\n",
    "        ct += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9542f5-70d9-4cc7-af70-821ce8c2b239",
   "metadata": {},
   "source": [
    "### Etape 4 : Choisir une fonction coût"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc3772a-09ec-4085-96aa-8b0635e15fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(wineModel.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65919506-ceec-4f9b-b0d8-947fa8fbbc54",
   "metadata": {},
   "source": [
    "### Training & Test\n",
    "\n",
    "C'est le moment de s'amuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0be17d25-af7e-4ffe-a4f9-e8d717426070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with optimizer: SGD, learning rate: 0.001\n",
      "Epoch  training loss   validation loss   accuracy        best train loss      best validation loss   best accuracy  \n",
      "1      1.13142612      1.12034976        35.0            1.13142612           1.12034976             35.0           \n",
      "2      1.13005664      1.11911994        35.0            1.13005664           1.11911994             35.0           \n",
      "3      1.12863396      1.11789644        35.0            1.12863396           1.11789644             35.0           \n",
      "4      1.12729353      1.11667901        40.0            1.12729353           1.11667901             40.0           \n",
      "5      1.12589566      1.11547422        40.0            1.12589566           1.11547422             40.0           \n",
      "6      1.12453441      1.1142664         40.0            1.12453441           1.1142664              40.0           \n",
      "7      1.12318623      1.1130625         40.0            1.12318623           1.1130625              40.0           \n",
      "8      1.12181147      1.11186147        40.0            1.12181147           1.11186147             40.0           \n",
      "9      1.12045932      1.11067468        40.0            1.12045932           1.11067468             40.0           \n",
      "10     1.11909129      1.10949171        40.0            1.11909129           1.10949171             40.0           \n",
      "11     1.11775468      1.10831499        40.0            1.11775468           1.10831499             40.0           \n",
      "12     1.11641375      1.10714382        40.0            1.11641375           1.10714382             40.0           \n",
      "13     1.1150948       1.10597551        40.0            1.1150948            1.10597551             40.0           \n",
      "14     1.113789        1.10479915        40.0            1.113789             1.10479915             40.0           \n",
      "15     1.11244969      1.10362023        40.0            1.11244969           1.10362023             40.0           \n",
      "16     1.11114019      1.10244721        40.0            1.11114019           1.10244721             40.0           \n",
      "17     1.10982283      1.10127711        40.0            1.10982283           1.10127711             40.0           \n",
      "18     1.10852762      1.10011357        40.0            1.10852762           1.10011357             40.0           \n",
      "19     1.10723776      1.09895849        40.0            1.10723776           1.09895849             40.0           \n",
      "20     1.10594104      1.09780312        40.0            1.10594104           1.09780312             40.0           \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory best_models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m         learning(EPOCHS, wineModel, criterion, optimizer, BATCH_SIZE, train_loader, val_loader, writer)\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# Save the best model for the current configuration\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_models/best_model_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrun_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# learning(20, wineModel, criterion, optimizer, BATCH_SIZE, train_loader, val_loader)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# torch.save(wineModel.state_dict(), \"best_model.pth\")\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print(len(val_loader.dataset))\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Testmodel(\"best_model.pth\", criterion, test_loader)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:627\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    624\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    628\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:501\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:472\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory best_models does not exist."
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "LEARNING_RATES = [0.001, 0.01]\n",
    "OPTIMIZERS = {\n",
    "    \"SGD\": SGD,\n",
    "    \"Adagrad\": Adagrad,\n",
    "    \"Adam\": Adam\n",
    "}\n",
    "\n",
    "# Copy the initial model to ensure the same starting weights for each run\n",
    "initial_model_state = copy.deepcopy(wineModel.state_dict())\n",
    "\n",
    "# Loop through configurations and train model\n",
    "for optimizer_name, optimizer_class in OPTIMIZERS.items():\n",
    "    for lr in LEARNING_RATES:\n",
    "        # Reset the model to initial state\n",
    "        wineModel.load_state_dict(initial_model_state)\n",
    "\n",
    "        # Create a new instance of the optimizer for each run\n",
    "        optimizer = optimizer_class(wineModel.parameters(), lr=lr)\n",
    "\n",
    "        # Create a SummaryWriter with a unique run name\n",
    "        run_name = f\"optimizer={optimizer_name}_lr={lr}_epochs={EPOCHS}\"\n",
    "        writer = SummaryWriter(log_dir=f'runs/{run_name}')\n",
    "\n",
    "        print(f\"Training with optimizer: {optimizer_name}, learning rate: {lr}\")\n",
    "        \n",
    "        # Train the model with the current configuration\n",
    "        learning(EPOCHS, wineModel, criterion, optimizer, BATCH_SIZE, train_loader, val_loader, writer)\n",
    "\n",
    "        # Save the best model for the current configuration\n",
    "        torch.save(wineModel.state_dict(), f\"best_models/best_model_{run_name}.pth\")\n",
    "\n",
    "# learning(20, wineModel, criterion, optimizer, BATCH_SIZE, train_loader, val_loader)\n",
    "# torch.save(wineModel.state_dict(), \"best_model.pth\")\n",
    "# print(len(val_loader.dataset))\n",
    "# Testmodel(\"best_model.pth\", criterion, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b747b-8dfe-4257-b1f4-338163b53aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da8f48b-bcd3-4c04-bb7e-8bd456669178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc3d3d-816a-4328-adc4-b864e0c36aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
